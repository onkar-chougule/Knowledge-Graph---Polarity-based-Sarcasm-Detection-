{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MXMOWPpL8mu0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize \n",
    "from nltk.util import ngrams\n",
    "from nltk import bigrams\n",
    "from tqdm import tqdm\n",
    "import string  \n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "# add appropriate words that will be ignored in the analysis\n",
    "ADDITIONAL_STOPWORDS = ['covfefe']\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk import pos_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate_files/train_data_for_amazon_sarcasm_detection.pkl', \"rb\") as fh:\n",
    "    dft = pickle.load(fh)\n",
    "with open('intermediate_files/test_data_for_amazon_sarcasm_detection.pkl', \"rb\") as fh:\n",
    "    dftest = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram, Trigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pos(dft):\n",
    "    \n",
    "    pos_tag_list=[]\n",
    "    for doc in tqdm(dft['review']):\n",
    "        sents=sent_tokenize(str(doc))\n",
    "        pos_doc=[]\n",
    "        for sent in sents:\n",
    "            sent=sent.lower()\n",
    "            sent=re.sub(r\"[^A-Za-z ]\",'',sent)\n",
    "            words=word_tokenize(sent)\n",
    "            for word in words:\n",
    "                pos_label=pos_tag(word_tokenize(word))[0][1]\n",
    "                pos_doc.append(pos_label)\n",
    "        joined=' '.join(pos_doc)\n",
    "        pos_tag_list.append(joined)\n",
    "    dft['pos_tags']=pos_tag_list\n",
    "    \n",
    "    return dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AGgfdiJv7HWE"
   },
   "outputs": [],
   "source": [
    "def frequency_counter(df,word_gram,pos):\n",
    "    \n",
    "    dft=df.copy()\n",
    "    c_vec = CountVectorizer(ngram_range=(word_gram,word_gram))\n",
    "\n",
    "    if pos:\n",
    "        all_pos_tags=[]\n",
    "        for doc in tqdm(dft['pos_tags']):\n",
    "            all_pos_tags.append(doc)\n",
    "        ngrams = c_vec.fit_transform(all_pos_tags)\n",
    "\n",
    "    else:\n",
    "        all_sentences=[]\n",
    "        for doc in dft['review']:\n",
    "            sents=sent_tokenize(str(doc))\n",
    "            for sent in sents:\n",
    "                all_sentences.append(sent)\n",
    "        sentdf=pd.DataFrame(all_sentences)\n",
    "        sentdf.columns=['sentences']\n",
    "        ngrams = c_vec.fit_transform(sentdf['sentences'])\n",
    "\n",
    "    \n",
    "    # matrix of ngrams\n",
    "    A_sub1 = csr_matrix(ngrams[:, :].sum(axis=0))\n",
    "    counts=A_sub1.toarray()\n",
    "    counts=counts.squeeze()\n",
    "    vocab = c_vec.vocabulary_\n",
    "    df_ngram = pd.DataFrame(sorted([(counts[i],k) for k,i in vocab.items()], reverse=True)).rename(columns={0: 'frequency', 1:'ngram'})\n",
    "    # df_ngram.head()\n",
    "\n",
    "    __freq={}\n",
    "    for key, fr in zip(df_ngram['ngram'],df_ngram['frequency']):\n",
    "        __freq[key]=fr\n",
    "    return __freq\n",
    "\n",
    "def calc_word_gram_sets_and_scores(df,word_gram,pos=False):\n",
    "    # freq of all bigram\n",
    "    dfs=df.loc[df['label']==1]\n",
    "    dfns=df.loc[df['label']==0]\n",
    "    if pos:\n",
    "        all_freq=frequency_counter(df,word_gram,pos)\n",
    "        sarc_freq=frequency_counter(dfs,word_gram,pos)\n",
    "        non_sarc_freq=frequency_counter(dfns,word_gram,pos)\n",
    "        required='pos_tags'\n",
    "    else:\n",
    "        all_freq=frequency_counter(df,word_gram,pos)\n",
    "        sarc_freq=frequency_counter(dfs,word_gram,pos)\n",
    "        non_sarc_freq=frequency_counter(dfns,word_gram,pos)\n",
    "        required='review'\n",
    "\n",
    "    word_gram_set_list=[]\n",
    "    word_gram_score=[]\n",
    "\n",
    "    for doc,label in tqdm(zip(df[required],df['label'])):\n",
    "        c_vec = CountVectorizer(ngram_range=(word_gram,word_gram))\n",
    "        bidoc_set=[]\n",
    "        tf_doc_sum=0\n",
    "        tf_all_sum=0\n",
    "        if pos:\n",
    "            doc_sentences=[doc]\n",
    "        else:\n",
    "            sents=sent_tokenize(str(doc))\n",
    "            \n",
    "            doc_sentences=[]\n",
    "            for sent in sents:\n",
    "                sent=sent.lower()\n",
    "                sent=re.sub(r\"[^A-Za-z ]\",'',sent)\n",
    "                doc_sentences.append(sent)\n",
    "\n",
    "        \n",
    "        ngrams = c_vec.fit_transform(doc_sentences)\n",
    "        vocab = c_vec.vocabulary_\n",
    "        for big,i in vocab.items():\n",
    "            # big=big[0]+' '+big[1]\n",
    "            bidoc_set.append(big)\n",
    "            if label==1:\n",
    "                try:\n",
    "                    tf_doc_sum+=sarc_freq[big]\n",
    "                    tf_all_sum+=all_freq[big]\n",
    "                except:\n",
    "                    tf_all_sum+=0\n",
    "                    tf_all_sum+=0\n",
    "            else:\n",
    "                try:\n",
    "                    tf_doc_sum+=non_sarc_freq[big]\n",
    "                    tf_all_sum+=all_freq[big]\n",
    "                except:\n",
    "                    tf_all_sum+=0\n",
    "                    tf_all_sum+=0\n",
    "        word_gram_set_list.append(bidoc_set)\n",
    "        word_gram_score.append(tf_doc_sum/tf_all_sum)\n",
    "    return word_gram_set_list,word_gram_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DUlmqyxXIe4Z",
    "outputId": "b150f01b-0eeb-4c2d-feb1-d9dc4c72e2bc"
   },
   "outputs": [],
   "source": [
    "def bi_tri_df(dft):\n",
    "    dft['bigram_set'],dft['bigram_wt']=calc_word_gram_sets_and_scores(dft,2)\n",
    "    dft['trigram_set'],dft['trigram_wt']=calc_word_gram_sets_and_scores(dft,3)\n",
    "\n",
    "    #POS\n",
    "    df_pos = extract_pos(dft)\n",
    "    dft['bigram_posSet'],dft['bigram_posWt']=calc_word_gram_sets_and_scores(df_pos,2,pos=True)\n",
    "    dft['trigram_posSet'],dft['trigram_posWt']=calc_word_gram_sets_and_scores(df_pos,3,pos=True)\n",
    "    dft.drop('pos_tags',inplace=True,axis=1)\n",
    "    \n",
    "    return dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "652it [00:01, 467.10it/s]\n",
      "652it [00:01, 497.66it/s]\n",
      "100%|██████████| 652/652 [00:43<00:00, 14.89it/s]\n",
      "100%|██████████| 652/652 [00:00<00:00, 115091.38it/s]\n",
      "100%|██████████| 326/326 [00:00<00:00, 340135.10it/s]\n",
      "100%|██████████| 326/326 [00:00<00:00, 123284.02it/s]\n",
      "652it [00:00, 924.07it/s]\n",
      "100%|██████████| 652/652 [00:00<00:00, 755229.55it/s]\n",
      "100%|██████████| 326/326 [00:00<00:00, 641342.92it/s]\n",
      "100%|██████████| 326/326 [00:00<00:00, 435876.03it/s]\n",
      "652it [00:00, 767.15it/s] \n"
     ]
    }
   ],
   "source": [
    "wt_bi_tri = bi_tri_df(dft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>bigram_set</th>\n",
       "      <th>bigram_wt</th>\n",
       "      <th>trigram_set</th>\n",
       "      <th>trigram_wt</th>\n",
       "      <th>bigram_posSet</th>\n",
       "      <th>bigram_posWt</th>\n",
       "      <th>trigram_posSet</th>\n",
       "      <th>trigram_posWt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I simply had no idea how bad it is! I am a sta...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[simply had, had no, no idea, idea how, how ba...</td>\n",
       "      <td>0.597990</td>\n",
       "      <td>[simply had no, had no idea, no idea how, idea...</td>\n",
       "      <td>0.887160</td>\n",
       "      <td>[nn rb, rb vbd, vbd dt, dt nn, nn wrb, wrb jj,...</td>\n",
       "      <td>0.530869</td>\n",
       "      <td>[nn rb vbd, rb vbd dt, vbd dt nn, dt nn wrb, n...</td>\n",
       "      <td>0.534252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A searing indictment... This book is sure to b...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[searing indictment, this book, book is, is su...</td>\n",
       "      <td>0.572722</td>\n",
       "      <td>[this book is, book is sure, is sure to, sure ...</td>\n",
       "      <td>0.792049</td>\n",
       "      <td>[dt vbg, vbg nn, nn dt, dt nn, nn vbz, vbz nn,...</td>\n",
       "      <td>0.530087</td>\n",
       "      <td>[dt vbg nn, vbg nn dt, nn dt nn, dt nn vbz, nn...</td>\n",
       "      <td>0.535295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Another movie to ignore.... A perfect date mov...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[another movie, movie to, to ignore, ignore pe...</td>\n",
       "      <td>0.483996</td>\n",
       "      <td>[another movie to, movie to ignore, to ignore ...</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>[dt nn, nn to, to nn, nn dt, nn nn, nn rb, rb ...</td>\n",
       "      <td>0.470786</td>\n",
       "      <td>[dt nn to, nn to nn, to nn dt, nn dt nn, dt nn...</td>\n",
       "      <td>0.476395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Buy this phone !! I got my Droid Incredible in...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[buy this, this phone, got my, my droid, droid...</td>\n",
       "      <td>0.593645</td>\n",
       "      <td>[buy this phone, got my droid, my droid incred...</td>\n",
       "      <td>0.881159</td>\n",
       "      <td>[vb dt, dt nn, nn nn, nn vbd, vbd prp, prp nn,...</td>\n",
       "      <td>0.531483</td>\n",
       "      <td>[vb dt nn, dt nn nn, nn nn vbd, nn vbd prp, vb...</td>\n",
       "      <td>0.537226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mother &amp; daughter So far this is pretty boring...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[mother daughter, daughter so, so far, far thi...</td>\n",
       "      <td>0.591287</td>\n",
       "      <td>[mother daughter so, daughter so far, so far t...</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>[nn nn, nn rb, rb rb, rb dt, dt vbz, vbz rb, r...</td>\n",
       "      <td>0.535552</td>\n",
       "      <td>[nn nn rb, nn rb rb, rb rb dt, rb dt vbz, dt v...</td>\n",
       "      <td>0.536632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>Plays tracks in alphanumeric order This is a g...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[plays tracks, tracks in, in alphanumeric, alp...</td>\n",
       "      <td>0.581565</td>\n",
       "      <td>[plays tracks in, tracks in alphanumeric, in a...</td>\n",
       "      <td>0.738532</td>\n",
       "      <td>[nns nns, nns in, in nn, nn nn, nn dt, dt vbz,...</td>\n",
       "      <td>0.531140</td>\n",
       "      <td>[nns nns in, nns in nn, in nn nn, nn nn dt, nn...</td>\n",
       "      <td>0.534347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>be very CAREFUL!!!! This is good phone. It is ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[be very, very careful, this is, is good, good...</td>\n",
       "      <td>0.434718</td>\n",
       "      <td>[be very careful, this is good, is good phone,...</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>[vb rb, rb nn, nn dt, dt vbz, vbz jj, jj nn, n...</td>\n",
       "      <td>0.468014</td>\n",
       "      <td>[vb rb nn, rb nn dt, nn dt vbz, dt vbz jj, vbz...</td>\n",
       "      <td>0.465452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>Really like this movie and its sequeal 10.5 ap...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[really like, like this, this movie, movie and...</td>\n",
       "      <td>0.582956</td>\n",
       "      <td>[really like this, like this movie, this movie...</td>\n",
       "      <td>0.684783</td>\n",
       "      <td>[rb in, in dt, dt nn, nn cc, cc prp, prp nn, n...</td>\n",
       "      <td>0.532343</td>\n",
       "      <td>[rb in dt, in dt nn, dt nn cc, nn cc prp, cc p...</td>\n",
       "      <td>0.539769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>Early Album of 2010 Candidate I open this revi...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[early album, album of, of candidate, candidat...</td>\n",
       "      <td>0.610280</td>\n",
       "      <td>[early album of, album of candidate, of candid...</td>\n",
       "      <td>0.856511</td>\n",
       "      <td>[rb nn, nn in, in nn, nn nn, nn jj, jj dt, dt ...</td>\n",
       "      <td>0.531486</td>\n",
       "      <td>[rb nn in, nn in nn, in nn nn, nn nn jj, nn jj...</td>\n",
       "      <td>0.536329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>Excellent! Just had this for lunch right now a...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[just had, had this, this for, for lunch, lunc...</td>\n",
       "      <td>0.614078</td>\n",
       "      <td>[just had this, had this for, this for lunch, ...</td>\n",
       "      <td>0.810345</td>\n",
       "      <td>[nn rb, rb vbd, vbd dt, dt in, in nn, nn nn, r...</td>\n",
       "      <td>0.536772</td>\n",
       "      <td>[nn rb vbd, rb vbd dt, vbd dt in, dt in nn, in...</td>\n",
       "      <td>0.548057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>652 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review  rating  label  \\\n",
       "0    I simply had no idea how bad it is! I am a sta...     5.0      0   \n",
       "1    A searing indictment... This book is sure to b...     5.0      0   \n",
       "2    Another movie to ignore.... A perfect date mov...     1.0      1   \n",
       "3    Buy this phone !! I got my Droid Incredible in...     5.0      0   \n",
       "4    Mother & daughter So far this is pretty boring...     3.0      0   \n",
       "..                                                 ...     ...    ...   \n",
       "647  Plays tracks in alphanumeric order This is a g...     1.0      0   \n",
       "648  be very CAREFUL!!!! This is good phone. It is ...     1.0      1   \n",
       "649  Really like this movie and its sequeal 10.5 ap...     5.0      0   \n",
       "650  Early Album of 2010 Candidate I open this revi...     5.0      0   \n",
       "651  Excellent! Just had this for lunch right now a...     5.0      0   \n",
       "\n",
       "                                            bigram_set  bigram_wt  \\\n",
       "0    [simply had, had no, no idea, idea how, how ba...   0.597990   \n",
       "1    [searing indictment, this book, book is, is su...   0.572722   \n",
       "2    [another movie, movie to, to ignore, ignore pe...   0.483996   \n",
       "3    [buy this, this phone, got my, my droid, droid...   0.593645   \n",
       "4    [mother daughter, daughter so, so far, far thi...   0.591287   \n",
       "..                                                 ...        ...   \n",
       "647  [plays tracks, tracks in, in alphanumeric, alp...   0.581565   \n",
       "648  [be very, very careful, this is, is good, good...   0.434718   \n",
       "649  [really like, like this, this movie, movie and...   0.582956   \n",
       "650  [early album, album of, of candidate, candidat...   0.610280   \n",
       "651  [just had, had this, this for, for lunch, lunc...   0.614078   \n",
       "\n",
       "                                           trigram_set  trigram_wt  \\\n",
       "0    [simply had no, had no idea, no idea how, idea...    0.887160   \n",
       "1    [this book is, book is sure, is sure to, sure ...    0.792049   \n",
       "2    [another movie to, movie to ignore, to ignore ...    0.914286   \n",
       "3    [buy this phone, got my droid, my droid incred...    0.881159   \n",
       "4    [mother daughter so, daughter so far, so far t...    0.911765   \n",
       "..                                                 ...         ...   \n",
       "647  [plays tracks in, tracks in alphanumeric, in a...    0.738532   \n",
       "648  [be very careful, this is good, is good phone,...    0.815642   \n",
       "649  [really like this, like this movie, this movie...    0.684783   \n",
       "650  [early album of, album of candidate, of candid...    0.856511   \n",
       "651  [just had this, had this for, this for lunch, ...    0.810345   \n",
       "\n",
       "                                         bigram_posSet  bigram_posWt  \\\n",
       "0    [nn rb, rb vbd, vbd dt, dt nn, nn wrb, wrb jj,...      0.530869   \n",
       "1    [dt vbg, vbg nn, nn dt, dt nn, nn vbz, vbz nn,...      0.530087   \n",
       "2    [dt nn, nn to, to nn, nn dt, nn nn, nn rb, rb ...      0.470786   \n",
       "3    [vb dt, dt nn, nn nn, nn vbd, vbd prp, prp nn,...      0.531483   \n",
       "4    [nn nn, nn rb, rb rb, rb dt, dt vbz, vbz rb, r...      0.535552   \n",
       "..                                                 ...           ...   \n",
       "647  [nns nns, nns in, in nn, nn nn, nn dt, dt vbz,...      0.531140   \n",
       "648  [vb rb, rb nn, nn dt, dt vbz, vbz jj, jj nn, n...      0.468014   \n",
       "649  [rb in, in dt, dt nn, nn cc, cc prp, prp nn, n...      0.532343   \n",
       "650  [rb nn, nn in, in nn, nn nn, nn jj, jj dt, dt ...      0.531486   \n",
       "651  [nn rb, rb vbd, vbd dt, dt in, in nn, nn nn, r...      0.536772   \n",
       "\n",
       "                                        trigram_posSet  trigram_posWt  \n",
       "0    [nn rb vbd, rb vbd dt, vbd dt nn, dt nn wrb, n...       0.534252  \n",
       "1    [dt vbg nn, vbg nn dt, nn dt nn, dt nn vbz, nn...       0.535295  \n",
       "2    [dt nn to, nn to nn, to nn dt, nn dt nn, dt nn...       0.476395  \n",
       "3    [vb dt nn, dt nn nn, nn nn vbd, nn vbd prp, vb...       0.537226  \n",
       "4    [nn nn rb, nn rb rb, rb rb dt, rb dt vbz, dt v...       0.536632  \n",
       "..                                                 ...            ...  \n",
       "647  [nns nns in, nns in nn, in nn nn, nn nn dt, nn...       0.534347  \n",
       "648  [vb rb nn, rb nn dt, nn dt vbz, dt vbz jj, vbz...       0.465452  \n",
       "649  [rb in dt, in dt nn, dt nn cc, nn cc prp, cc p...       0.539769  \n",
       "650  [rb nn in, nn in nn, in nn nn, nn nn jj, nn jj...       0.536329  \n",
       "651  [nn rb vbd, rb vbd dt, vbd dt in, dt in nn, in...       0.548057  \n",
       "\n",
       "[652 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt_bi_tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_bi_tri.to_pickle('intermediate_files/Train_bi_tri_wt.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram, Trigram Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KocYO3jZ_mln"
   },
   "outputs": [],
   "source": [
    "def calc_word_gram_sets_test(df,word_gram,pos=False):\n",
    "    # freq of all bigram\n",
    "\n",
    "    if pos:\n",
    "\n",
    "        required='pos_tags'\n",
    "    else:\n",
    "\n",
    "        required='review'\n",
    "\n",
    "    word_gram_set_list=[]\n",
    "\n",
    "    for doc,label in tqdm(zip(df[required],df['label'])):\n",
    "        c_vec = CountVectorizer(ngram_range=(word_gram,word_gram))\n",
    "        bidoc_set=[]\n",
    "\n",
    "        if pos:\n",
    "            doc_sentences=[doc]\n",
    "        else:\n",
    "            sents=sent_tokenize(str(doc))\n",
    "            \n",
    "            doc_sentences=[]\n",
    "            for sent in sents:\n",
    "                sent=sent.lower()\n",
    "                sent=re.sub(r\"[^A-Za-z ]\",'',sent)\n",
    "                doc_sentences.append(sent)\n",
    "                \n",
    "        ngrams = c_vec.fit_transform(doc_sentences)\n",
    "        vocab = c_vec.vocabulary_\n",
    "        for big,i in vocab.items():\n",
    "            # big=big[0]+' '+big[1]\n",
    "            bidoc_set.append(big)\n",
    "\n",
    "        word_gram_set_list.append(bidoc_set)\n",
    "\n",
    "    return word_gram_set_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "vSdTWiycEdAm",
    "outputId": "e86bbd05-4470-4cbd-9909-d111e8db66e5"
   },
   "outputs": [],
   "source": [
    "def test_bi_tri(dftest):\n",
    "    dftest['bigram_set']=calc_word_gram_sets_test(dftest,2)\n",
    "    dftest['trigram_set']=calc_word_gram_sets_test(dftest,3)\n",
    "\n",
    "    df_pos = extract_pos(dftest)\n",
    "    dftest['bigram_posSet']=calc_word_gram_sets_test(df_pos,2,pos=True)\n",
    "    dftest['trigram_posSet']=calc_word_gram_sets_test(df_pos,3,pos=True)\n",
    "    \n",
    "    dftest.drop('pos_tags',inplace=True,axis=1)\n",
    "    \n",
    "    return dftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uvYUNsmrE2VG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218it [00:00, 317.14it/s]\n",
      "218it [00:00, 365.98it/s]\n",
      "100%|██████████| 218/218 [00:20<00:00, 10.45it/s]\n",
      "218it [00:00, 789.58it/s]\n",
      "218it [00:00, 694.75it/s]\n"
     ]
    }
   ],
   "source": [
    "df_test= test_bi_tri(dftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest.to_pickle('intermediate_files/Test_bi_tri.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>bigram_set</th>\n",
       "      <th>trigram_set</th>\n",
       "      <th>bigram_posSet</th>\n",
       "      <th>trigram_posSet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great Alyssa, but rather boring movie This is ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[great alyssa, alyssa but, but rather, rather ...</td>\n",
       "      <td>[great alyssa but, alyssa but rather, but rath...</td>\n",
       "      <td>[jj nn, nn cc, cc rb, rb nn, nn nn, nn dt, dt ...</td>\n",
       "      <td>[jj nn cc, nn cc rb, cc rb nn, rb nn nn, nn nn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worst ever If this was written by Danielle Ste...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[worst ever, ever if, if this, this was, was w...</td>\n",
       "      <td>[worst ever if, ever if this, if this was, thi...</td>\n",
       "      <td>[jjs rb, rb in, in dt, dt vbd, vbd vbn, vbn in...</td>\n",
       "      <td>[jjs rb in, rb in dt, in dt vbd, dt vbd vbn, v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hasselhoff Me!  Please! One was having a parti...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[hasselhoff me, one was, was having, having pa...</td>\n",
       "      <td>[one was having, was having particularly, havi...</td>\n",
       "      <td>[nn prp, prp nn, nn cd, cd vbd, vbd vbg, vbg d...</td>\n",
       "      <td>[nn prp nn, prp nn cd, nn cd vbd, cd vbd vbg, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One Friday, Without the Milk He always brought...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[one friday, friday without, without the, the ...</td>\n",
       "      <td>[one friday without, friday without the, witho...</td>\n",
       "      <td>[cd nn, nn in, in dt, dt nn, nn prp, prp rb, r...</td>\n",
       "      <td>[cd nn in, nn in dt, in dt nn, dt nn prp, nn p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this. sucked. bad. one word: punctuation. ok, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[one word, word punctuation, ok folks, folks h...</td>\n",
       "      <td>[one word punctuation, ok folks heres, folks h...</td>\n",
       "      <td>[dt vbn, vbn jj, jj cd, cd nn, nn nn, nn nns, ...</td>\n",
       "      <td>[dt vbn jj, vbn jj cd, jj cd nn, cd nn nn, nn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>If dragons could wear t-shirts, this is the on...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[if dragons, dragons could, could wear, wear t...</td>\n",
       "      <td>[if dragons could, dragons could wear, could w...</td>\n",
       "      <td>[in nns, nns md, md nn, nn nns, nns dt, dt vbz...</td>\n",
       "      <td>[in nns md, nns md nn, md nn nns, nn nns dt, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>Horrible Product this product only got one sta...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[horrible product, product this, this product,...</td>\n",
       "      <td>[horrible product this, product this product, ...</td>\n",
       "      <td>[jj nn, nn dt, dt nn, nn rb, rb vbd, vbd cd, c...</td>\n",
       "      <td>[jj nn dt, nn dt nn, dt nn rb, nn rb vbd, rb v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>BDP-S560 vs Panasonic DMP-BD80 There seems to ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[bdps vs, vs panasonic, panasonic dmpbd, dmpbd...</td>\n",
       "      <td>[bdps vs panasonic, vs panasonic dmpbd, panaso...</td>\n",
       "      <td>[nn nn, nn rb, rb vbz, vbz to, to vb, vb dt, d...</td>\n",
       "      <td>[nn nn nn, nn nn rb, nn rb vbz, rb vbz to, vbz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Great Gift... I know nothing about guitars so ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[great gift, know nothing, nothing about, abou...</td>\n",
       "      <td>[know nothing about, nothing about guitars, ab...</td>\n",
       "      <td>[jj nn, nn nn, nn vb, vb nn, nn in, in nns, nn...</td>\n",
       "      <td>[jj nn nn, nn nn vb, nn vb nn, vb nn in, nn in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>Finding the Real Treasure Most people born and...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[finding the, the real, real treasure, treasur...</td>\n",
       "      <td>[finding the real, the real treasure, real tre...</td>\n",
       "      <td>[vbg dt, dt jj, jj nn, nn jjs, jjs nns, nns nn...</td>\n",
       "      <td>[vbg dt jj, dt jj nn, jj nn jjs, nn jjs nns, j...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>218 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review  rating  label  \\\n",
       "0    Great Alyssa, but rather boring movie This is ...     2.0      0   \n",
       "1    worst ever If this was written by Danielle Ste...     1.0      1   \n",
       "2    Hasselhoff Me!  Please! One was having a parti...     5.0      1   \n",
       "3    One Friday, Without the Milk He always brought...     3.0      1   \n",
       "4    this. sucked. bad. one word: punctuation. ok, ...     1.0      1   \n",
       "..                                                 ...     ...    ...   \n",
       "213  If dragons could wear t-shirts, this is the on...     5.0      1   \n",
       "214  Horrible Product this product only got one sta...     1.0      1   \n",
       "215  BDP-S560 vs Panasonic DMP-BD80 There seems to ...     5.0      0   \n",
       "216  Great Gift... I know nothing about guitars so ...     4.0      0   \n",
       "217  Finding the Real Treasure Most people born and...     5.0      0   \n",
       "\n",
       "                                            bigram_set  \\\n",
       "0    [great alyssa, alyssa but, but rather, rather ...   \n",
       "1    [worst ever, ever if, if this, this was, was w...   \n",
       "2    [hasselhoff me, one was, was having, having pa...   \n",
       "3    [one friday, friday without, without the, the ...   \n",
       "4    [one word, word punctuation, ok folks, folks h...   \n",
       "..                                                 ...   \n",
       "213  [if dragons, dragons could, could wear, wear t...   \n",
       "214  [horrible product, product this, this product,...   \n",
       "215  [bdps vs, vs panasonic, panasonic dmpbd, dmpbd...   \n",
       "216  [great gift, know nothing, nothing about, abou...   \n",
       "217  [finding the, the real, real treasure, treasur...   \n",
       "\n",
       "                                           trigram_set  \\\n",
       "0    [great alyssa but, alyssa but rather, but rath...   \n",
       "1    [worst ever if, ever if this, if this was, thi...   \n",
       "2    [one was having, was having particularly, havi...   \n",
       "3    [one friday without, friday without the, witho...   \n",
       "4    [one word punctuation, ok folks heres, folks h...   \n",
       "..                                                 ...   \n",
       "213  [if dragons could, dragons could wear, could w...   \n",
       "214  [horrible product this, product this product, ...   \n",
       "215  [bdps vs panasonic, vs panasonic dmpbd, panaso...   \n",
       "216  [know nothing about, nothing about guitars, ab...   \n",
       "217  [finding the real, the real treasure, real tre...   \n",
       "\n",
       "                                         bigram_posSet  \\\n",
       "0    [jj nn, nn cc, cc rb, rb nn, nn nn, nn dt, dt ...   \n",
       "1    [jjs rb, rb in, in dt, dt vbd, vbd vbn, vbn in...   \n",
       "2    [nn prp, prp nn, nn cd, cd vbd, vbd vbg, vbg d...   \n",
       "3    [cd nn, nn in, in dt, dt nn, nn prp, prp rb, r...   \n",
       "4    [dt vbn, vbn jj, jj cd, cd nn, nn nn, nn nns, ...   \n",
       "..                                                 ...   \n",
       "213  [in nns, nns md, md nn, nn nns, nns dt, dt vbz...   \n",
       "214  [jj nn, nn dt, dt nn, nn rb, rb vbd, vbd cd, c...   \n",
       "215  [nn nn, nn rb, rb vbz, vbz to, to vb, vb dt, d...   \n",
       "216  [jj nn, nn nn, nn vb, vb nn, nn in, in nns, nn...   \n",
       "217  [vbg dt, dt jj, jj nn, nn jjs, jjs nns, nns nn...   \n",
       "\n",
       "                                        trigram_posSet  \n",
       "0    [jj nn cc, nn cc rb, cc rb nn, rb nn nn, nn nn...  \n",
       "1    [jjs rb in, rb in dt, in dt vbd, dt vbd vbn, v...  \n",
       "2    [nn prp nn, prp nn cd, nn cd vbd, cd vbd vbg, ...  \n",
       "3    [cd nn in, nn in dt, in dt nn, dt nn prp, nn p...  \n",
       "4    [dt vbn jj, vbn jj cd, jj cd nn, cd nn nn, nn ...  \n",
       "..                                                 ...  \n",
       "213  [in nns md, nns md nn, md nn nns, nn nns dt, n...  \n",
       "214  [jj nn dt, nn dt nn, dt nn rb, nn rb vbd, rb v...  \n",
       "215  [nn nn nn, nn nn rb, nn rb vbz, rb vbz to, vbz...  \n",
       "216  [jj nn nn, nn nn vb, nn vb nn, vb nn in, nn in...  \n",
       "217  [vbg dt jj, dt jj nn, jj nn jjs, nn jjs nns, j...  \n",
       "\n",
       "[218 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "functions_for_finding_bi_tri_pos_weights_sets.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
